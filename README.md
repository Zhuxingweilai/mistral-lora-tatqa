## 当AI模型落地遇上现实工程：为什么没有Function Calling的应用只是“智能幻觉”？

——从一次大模型微调实习说起

* * *

### 引言

当越来越多企业投身于“大模型+”的建设热潮中，我们发现了一个令人担忧的趋势：很多AI项目在启动时满怀期待，但最后却落得“模型很大，业务没动”的尴尬局面。是什么阻碍了AI真正成为一个能用、好用、值得用的生产力工具？

近期，我参与了一个基于LLaMA-Factory的大语言模型微调项目，负责从原始数据预处理、环境部署、模型训练到测试验证的全过程。在深入实操中，我才意识到一个被严重低估的问题：**AI应用的工程能力，特别是Function Calling、上下文处理、工具调度等能力，决定了一个AI项目是“可上线”，还是“写个PPT”**。

本文将结合我的实习项目，深入探讨大模型微调中隐藏的工程挑战，并解释为什么我们不能再仅靠“让大模型自己想”，而要将Function Calling作为落地的第一原则。

* * *

## 一、Function Calling不是“高级选项”，而是AI工程的入门必修课

在项目初期，我们将目标定为训练一个可以基于宏观经济数据（如美联储利率、美元指数、中美CPI等）预测下月人民币汇率区间的模型，使用了LoRA微调方法、AutoModelForCausalLM结构，搭建了基于时间序列的instruction tuning数据集。

然而，我很快意识到：**模型本身能力再强，如果没有设计好调用逻辑，最终生成的只是“听起来像在帮你做事”的语言幻觉**。

> 举个例子：我们曾在训练过程中加入“发邮件”这类任务指令，但模型输出的只是“我将为您发送邮件”的自然语言——而非真正触发邮件发送API。这种“假动作”在实际应用中毫无意义。

没有Function Calling，模型就像一个有思想但没有手脚的助手，它能理解你说什么，却做不了任何事情。

* * *

## 二、工程化实战：为什么大多数AI项目“死在环境配置上”

部署LLaMA-Factory的过程让我真切体会到工程环境的复杂性——本地环境中，Transformers版本与CUDA冲突、PyTorch和TensorFlow之间资源争用，调一天不如云端点一下。

最终，我选择了云电脑（如Azure）为部署主阵地，利用其预设的比本地条件更好的GPU环境与镜像，大大加快了环境配置速度和每次训练的效率。这也让我意识到：**没有良好的工程环境，大模型项目就不可能走得远。**

而且，Function Calling不仅要求模型能“说出”调用指令，更要求它能在真实环境中“调得动”工具。这里的每一步都依赖稳定的API接口、数据结构一致性、异常处理机制等工程细节。工程做不好，Function Calling 就成了“嘴炮”。

* * *

## 三、指令微调不等于真正理解：大模型的“幻觉式调用”有多可怕？

在微调 `phi-4-mini-reasoning` 模型时，我发现即使 instruction tuning 结构合理，模型依旧容易犯几个“工程致命错”：

- **张冠李戴**：我们希望模型根据指令调用 get\_weather 和 send\_email 两个模拟API，结果模型生成的是完全不存在的函数名，如 get\_exchange\_rate。
- **参数瞎编**：用户只问“查北京明天天气”，模型却生成 get\_weather(city='火星')。
- **多轮失忆**：模型上一轮已获取天气，下一轮却忘记用户还想发邮件提醒，失去了任务连续性。

这些问题让我深刻体会到：**单纯靠微调无法解决Function Calling的核心问题**，你必须配合上下文记忆机制、工具调度模块（如LangChain Agent），才能让模型“既能理解，又能操作”。

* * *

## 四、可行的落地方式：微调 + Function Calling + Tool Router

本次实习虽然主要聚焦在模型微调，但也促使我思考：**如果未来继续开发一个完整AI系统，我们应该怎么做？**

我的初步答案是：

- 利用微调提升“调用意识”与语言生成合理性；
- 搭建Function Calling结构（如OpenAI函数调用格式、LLaMA-Factory或LangChain工具集成）；
- 加入参数校验与异常兜底机制，防止错误传播；
- 引入Memory模块支持跨轮连续调用；
- 使用多智能体结构，将子任务解耦分工，提升稳定性与可扩展性。

只有这样，AI模型才有可能从“懂你在说什么”，变成“帮你真的做成事”。

* * *

## 五、微调结果与效果

经过约 12 分钟的单卡 GPU 训练（315 步），模型的 loss 从接近 10 快速下降到 1 左右，并在后期趋于稳定（见下图）。

![](Files/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202025-08-03%20235737.png)<br>
这种趋势表明模型已经较好地掌握了财务表格问答的模式和推理逻辑。

- 训练过程可视化
  - 在 LLaMA Factory 的 WebUI 训练界面，可以实时看到 loss 曲线。起初模型几乎完全依赖随机猜测，loss 接近 10；而随着训练进行，曲线迅速下降，并在 150 步左右趋缓，说明模型在持续细化对任务细节的掌握。
- 虽然还没有进行验证集评测，但从 loss 曲线可以合理推测，模型已学习到财务报表问答的基本模式，包括：

  - 如何解析表格结构与文字说明
  - 如何根据年份、类别等条件筛选数据
  - 如何将提取结果按指定格式输出
  - 这意味着在相似任务上，它的回答应该比原始模型更加准确、稳定。

* * *

## 六、微调模型回答与原模型回答比对

在微调实验中，我们还对比了微调前后模型的回答差异（见下图）。可以明显看到，微调后的回答在形式上更精炼、直接，减少了冗余解释；而微调前的回答更完整、自解释，适合脱离上下文单独阅读。  

1. 原模型
   - ![](Files/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202025-08-11%20170448.png)
   - ![](Files/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202025-08-11%20170542.png)
2. 微调模型
   - ![](Files/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202025-08-11%20170500.png)
   - ![](Files/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202025-08-11%20170600.png)

这种差异与训练数据量、任务设计、指令结构都有关系。当数据更聚焦、模式更统一时，模型更倾向于生成短答；而泛化性更强的训练，会让模型保留更多补充信息。  
这也再次印证了一个现实：模型微调不仅影响“答什么”，还会影响“怎么答”。如果缺乏 Function Calling、上下文记忆等工程机制，即便模型回答再简洁，也可能只是“智能幻觉”，而不能真正完成任务。

* * *

## 结语：我们不是在训练模型，而是在建一套“智能系统基础设施”

本次实习让我认识到，AI项目的终点不再是“训练出一个好模型”，而是“构建一个可协作、可调用、可控制的智能系统”。

Function Calling、Tool Calling、Memory、Agent，这些不是加分项，而是智能应用时代的系统构件。  
只有工程落地能力过硬，AI才不只是一个说话的人，而是一个能完成任务的数字员工。

我想，这或许才是AI工程真正令人着迷的地方。
